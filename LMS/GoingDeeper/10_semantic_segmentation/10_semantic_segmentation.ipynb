{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greater-discount",
   "metadata": {},
   "source": [
    "## 프로젝트 : 개선된 U-Net 모델 만들기\n",
    "\n",
    "<img src=\"img/result.gif\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "\n",
    "### 실습목표\n",
    "---\n",
    "- 시맨틱 세그멘테이션 데이터셋을 전처리할 수 있습니다.\n",
    "- 시맨틱 세그멘테이션 모델을 만들고 학습할 수 있습니다.\n",
    "- 시맨틱 세그멘테이션 모델의 결과를 시각화할 수 있습니다.\n",
    "\n",
    "### 학습내용\n",
    "---\n",
    "1. 시맨틱 세그멘테이션 데이터셋\n",
    "2. 시맨틱 세그멘테이션 모델\n",
    "3. 시맨틱 세그멘테이션 모델 시각화\n",
    "\n",
    "<img src=\"img/GC-5-P-UNPP.max-800x600.png\" width=\"80%\" height=\"80%\">\n",
    "<center><span style=\"color:#aeaeae\">[KITTI 데이터셋 segmentation_rgb]</span></center>\n",
    "\n",
    "## Step 1. KITTI 데이터셋 수집과 구축\n",
    "\n",
    "다운받아 둔 KITTI 데이터에 data augmentation을 적용한 형태로 데이터셋을 구축합니다. 이때 주의할 점이 있습니다. U-Net++는 내부적인 메모리 사용량이 U-Net보다 꽤 많아집니다. 8GB의 GPU 메모리를 가진 모델의 경우 학습 데이터의 배치 사이즈를 16->4 로 줄여서 설정하시기를 권합니다.\n",
    "\n",
    "#### 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 라이브러리를 로드합니다. \n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from glob import glob\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-christian",
   "metadata": {},
   "source": [
    "#### Augmentation 기법을 확률적으로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import  HorizontalFlip, RandomSizedCrop, Compose, OneOf, Resize\n",
    "\n",
    "def build_augmentation(is_train=True):\n",
    "  if is_train:    # 훈련용 데이터일 경우\n",
    "    return Compose([\n",
    "                    HorizontalFlip(p=0.5),    # 50%의 확률로 좌우대칭\n",
    "                    RandomSizedCrop(         # 50%의 확률로 RandomSizedCrop\n",
    "                        min_max_height=(300, 370),\n",
    "                        w2h_ratio=370/1242,\n",
    "                        height=224,\n",
    "                        width=224,\n",
    "                        p=0.5\n",
    "                        ),\n",
    "                    Resize(              # 입력이미지를 224X224로 resize\n",
    "                        width=224,\n",
    "                        height=224\n",
    "                        )\n",
    "                    ])\n",
    "  return Compose([      # 테스트용 데이터일 경우에는 224X224로 resize만 수행합니다. \n",
    "                Resize(\n",
    "                    width=224,\n",
    "                    height=224\n",
    "                    )\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = os.getenv('HOME')+'/aiffel/semantic_segmentation/data/training'\n",
    "\n",
    "augmentation = build_augmentation()\n",
    "input_images = glob(os.path.join(dir_path, \"image_2\", \"*.png\"))\n",
    "\n",
    "# 훈련 데이터셋에서 5개만 가져와 augmentation을 적용해 봅시다.  \n",
    "plt.figure(figsize=(12, 20))\n",
    "for i in range(5):\n",
    "    image = imread(input_images[i]) \n",
    "    image_data = {\"image\":image}\n",
    "    resized = augmentation(**image_data, is_train=False)\n",
    "    processed = augmentation(**image_data)\n",
    "    plt.subplot(5, 2, 2*i+1)\n",
    "    plt.imshow(resized[\"image\"])  # 왼쪽이 원본이미지\n",
    "    plt.subplot(5, 2, 2*i+2)\n",
    "    plt.imshow(processed[\"image\"])  # 오른쪽이 augment된 이미지\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-taste",
   "metadata": {},
   "source": [
    "#### `tf.keras.utils.Sequence`를 상속받은 `generator` 형태로 데이터를 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittiGenerator(tf.keras.utils.Sequence):\n",
    "  '''\n",
    "  KittiGenerator는 tf.keras.utils.Sequence를 상속받습니다.\n",
    "  우리가 KittiDataset을 원하는 방식으로 preprocess하기 위해서 Sequnce를 커스텀해 사용합니다.\n",
    "  '''\n",
    "  def __init__(self, \n",
    "               dir_path,\n",
    "               batch_size=4,\n",
    "               img_size=(224, 224, 3),\n",
    "               output_size=(224, 224),\n",
    "               is_train=True,\n",
    "               augmentation=None):\n",
    "    '''\n",
    "    dir_path: dataset의 directory path입니다.\n",
    "    batch_size: batch_size입니다.\n",
    "    img_size: preprocess에 사용할 입력이미지의 크기입니다.\n",
    "    output_size: ground_truth를 만들어주기 위한 크기입니다.\n",
    "    is_train: 이 Generator가 학습용인지 테스트용인지 구분합니다.\n",
    "    augmentation: 적용하길 원하는 augmentation 함수를 인자로 받습니다.\n",
    "    '''\n",
    "    self.dir_path = dir_path\n",
    "    self.batch_size = batch_size\n",
    "    self.is_train = is_train\n",
    "    self.dir_path = dir_path\n",
    "    self.augmentation = augmentation\n",
    "    self.img_size = img_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    # load_dataset()을 통해서 kitti dataset의 directory path에서 라벨과 이미지를 확인합니다.\n",
    "    self.data = self.load_dataset()\n",
    "\n",
    "  def load_dataset(self):\n",
    "    # kitti dataset에서 필요한 정보(이미지 경로 및 라벨)를 directory에서 확인하고 로드하는 함수입니다.\n",
    "    # 이때 is_train에 따라 test set을 분리해서 load하도록 해야합니다.\n",
    "    input_images = glob(os.path.join(self.dir_path, \"image_2\", \"*.png\"))\n",
    "    label_images = glob(os.path.join(self.dir_path, \"semantic\", \"*.png\"))\n",
    "    input_images.sort()\n",
    "    label_images.sort()\n",
    "    assert len(input_images) == len(label_images)\n",
    "    data = [ _ for _ in zip(input_images, label_images)]\n",
    "\n",
    "    if self.is_train:\n",
    "      return data[:-30]\n",
    "    return data[-30:]\n",
    "    \n",
    "  def __len__(self):\n",
    "    # Generator의 length로서 전체 dataset을 batch_size로 나누고 소숫점 첫째자리에서 올림한 값을 반환합니다.\n",
    "    return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # 입력과 출력을 만듭니다.\n",
    "    # 입력은 resize및 augmentation이 적용된 input image이고 \n",
    "    # 출력은 semantic label입니다.\n",
    "    batch_data = self.data[\n",
    "                           index*self.batch_size:\n",
    "                           (index + 1)*self.batch_size\n",
    "                           ]\n",
    "    inputs = np.zeros([self.batch_size, *self.img_size])\n",
    "    outputs = np.zeros([self.batch_size, *self.output_size])\n",
    "        \n",
    "    for i, data in enumerate(batch_data):\n",
    "      input_img_path, output_path = data\n",
    "      _input = imread(input_img_path)\n",
    "      _output = imread(output_path)\n",
    "      _output = (_output==7).astype(np.uint8)*1\n",
    "      data = {\n",
    "          \"image\": _input,\n",
    "          \"mask\": _output,\n",
    "          }\n",
    "      augmented = self.augmentation(**data)\n",
    "      inputs[i] = augmented[\"image\"]/255\n",
    "      outputs[i] = augmented[\"mask\"]\n",
    "      return inputs, outputs\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    # 한 epoch가 끝나면 실행되는 함수입니다. 학습중인 경우에 순서를 random shuffle하도록 적용한 것을 볼 수 있습니다.\n",
    "    self.indexes = np.arange(len(self.data))\n",
    "    if self.is_train == True :\n",
    "      np.random.shuffle(self.indexes)\n",
    "      return self.indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = build_augmentation()\n",
    "test_preproc = build_augmentation(is_train=False)\n",
    "        \n",
    "train_generator = KittiGenerator(\n",
    "    dir_path, \n",
    "    augmentation=augmentation,\n",
    ")\n",
    "\n",
    "test_generator = KittiGenerator(\n",
    "    dir_path, \n",
    "    augmentation=test_preproc,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-magic",
   "metadata": {},
   "source": [
    "## Step 2. U-Net++ 모델의 구현\n",
    "\n",
    "U-Net의 모델 구조와 소스코드를 면밀히 비교해 보다 보면, U-Net++를 어떻게 구현할 수 있을지에 대한 방안을 떠올릴 수 있을 것입니다. 이 과정을 통해 U-Net 자체에 대한 이해도도 증진될 것입니다.\n",
    "그 외 적절히 U-Net의 백본 구조, 기타 파라미터 변경 등을 통해 추가적인 성능 향상이 가능할 수도 있습니다.\n",
    "\n",
    "### 모델 구조 만들기 (U-Net 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(224, 224, 3)):\n",
    "  inputs = Input(input_shape)\n",
    "\n",
    "  #Contracting Path\n",
    "  conv1 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(inputs)\n",
    "  conv1 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv1)\n",
    "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "  conv2 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool1)\n",
    "  conv2 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv2)\n",
    "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "  conv3 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool2)\n",
    "  conv3 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv3)\n",
    "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "  conv4 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool3)\n",
    "  conv4 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv4)\n",
    "  drop4 = Dropout(0.5)(conv4)\n",
    "  pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "  conv5 = Conv2D(1024, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool4)  \n",
    "  conv5 = Conv2D(1024, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv5)\n",
    "\n",
    "  #Expanding Path\n",
    "  drop5 = Dropout(0.5)(conv5)\n",
    "  up6 = Conv2DTranspose(512, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(drop5)\n",
    "  merge6 = concatenate([drop4,up6], axis = 3)\n",
    "  conv6 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge6)\n",
    "  conv6 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv6)\n",
    "  up7 = Conv2DTranspose(256, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv6)\n",
    "  merge7 = concatenate([conv3,up7], axis = 3)\n",
    "  conv7 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge7)\n",
    "  conv7 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv7)\n",
    "  up8 = Conv2DTranspose(128, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv7)\n",
    "  merge8 = concatenate([conv2,up8], axis = 3)\n",
    "  conv8 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge8)\n",
    "  conv8 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv8)\n",
    "  up9 = Conv2DTranspose(64, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv8)\n",
    "  merge9 = concatenate([conv1,up9], axis = 3)\n",
    "  conv9 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge9)\n",
    "  conv9 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv9)  \n",
    "  conv9 = Conv2D(2, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv9)     \n",
    "  conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "  model = Model(inputs = inputs, outputs = conv10)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-concrete",
   "metadata": {},
   "source": [
    "### 모델 구조 만들기 (U-Net++ 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(224, 224, 3)):\n",
    "  inputs = Input(input_shape)\n",
    "\n",
    "  # X_,0\n",
    "  conv00 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(inputs)\n",
    "  conv00 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv00)\n",
    "  pool00 = MaxPooling2D(pool_size=(2, 2))(conv00)\n",
    "  conv10 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool00)\n",
    "  conv10 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv10)\n",
    "  pool10 = MaxPooling2D(pool_size=(2, 2))(conv10)\n",
    "  conv20 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool10)\n",
    "  conv20 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv20)\n",
    "  pool20 = MaxPooling2D(pool_size=(2, 2))(conv20)\n",
    "  conv30 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool20)\n",
    "  conv30 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv30)\n",
    "#   drop30 = Dropout(0.5)(conv30)\n",
    "  pool30 = MaxPooling2D(pool_size=(2, 2))(conv30)\n",
    "\n",
    "  conv40 = Conv2D(1024, 3, activation='relu', padding='same',kernel_initializer='he_normal')(pool30)  \n",
    "  conv40 = Conv2D(1024, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv40)\n",
    "\n",
    "  # X_,1\n",
    "  up01 = Conv2DTranspose(64, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv10)\n",
    "  merge01 = concatenate([conv00,up01], axis = 3)\n",
    "  conv01 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge01)\n",
    "  conv01 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv01)\n",
    "  up11 = Conv2DTranspose(128, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv20)\n",
    "  merge11 = concatenate([conv10,up11], axis = 3)\n",
    "  conv11 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge11)\n",
    "  conv11 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv11)\n",
    "  up21 = Conv2DTranspose(256, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv30)\n",
    "  merge21 = concatenate([conv20,up21], axis = 3)\n",
    "  conv21 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge21)\n",
    "  conv21 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv21)\n",
    "  up31 = Conv2DTranspose(512, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv40)\n",
    "  merge31 = concatenate([conv30,up31], axis = 3)\n",
    "  conv31 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge31)\n",
    "  conv31 = Conv2D(512, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv31)\n",
    "  \n",
    "  # X_,2\n",
    "  up02 = Conv2DTranspose(64, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv11)\n",
    "  merge02 = concatenate([conv00,conv01,up02], axis = 3)\n",
    "  conv02 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge02)\n",
    "  conv02 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv02)\n",
    "  up12 = Conv2DTranspose(128, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv21)\n",
    "  merge12 = concatenate([conv10,conv11,up12], axis = 3)\n",
    "  conv12 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge12)\n",
    "  conv11 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv12)\n",
    "  up22 = Conv2DTranspose(256, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv31)\n",
    "  merge22 = concatenate([conv20,conv21,up22], axis = 3)\n",
    "  conv22 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge22)\n",
    "  conv22 = Conv2D(256, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv22)\n",
    "  \n",
    "  # X_,3\n",
    "  up03 = Conv2DTranspose(64, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv12)\n",
    "  merge03 = concatenate([conv00,conv01,conv02,up03], axis = 3)\n",
    "  conv03 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge03)\n",
    "  conv03 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv03)\n",
    "  up13 = Conv2DTranspose(128, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv22)\n",
    "  merge13 = concatenate([conv10,conv11,conv12,up13], axis = 3)\n",
    "  conv13 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge13)\n",
    "  conv13 = Conv2D(128, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv13)\n",
    "\n",
    "  # X_,4\n",
    "  up04 = Conv2DTranspose(64, 2, activation='relu', strides=(2,2), kernel_initializer='he_normal')(conv13)\n",
    "  merge04 = concatenate([conv00,conv01,conv02,conv03,up04], axis = 3)\n",
    "  conv04 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(merge04)\n",
    "  conv04 = Conv2D(64, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv04)\n",
    "  conv04 = Conv2D(2, 3, activation='relu', padding='same',kernel_initializer='he_normal')(conv04)     \n",
    "  conv04 = Conv2D(1, 1, activation='sigmoid')(conv04)\n",
    "\n",
    "  model = Model(inputs = inputs, outputs = conv04)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-static",
   "metadata": {},
   "source": [
    "#### 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getenv('HOME')+'/aiffel/semantic_segmentation/seg_model_unet.h5'\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy')\n",
    "model.fit_generator(\n",
    "     generator=train_generator,\n",
    "     validation_data=test_generator,\n",
    "     steps_per_epoch=len(train_generator),\n",
    "     epochs=100,\n",
    " )\n",
    "\n",
    "model.save(model_path)  #학습한 모델을 저장해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-flexibility",
   "metadata": {},
   "source": [
    "## Step 3. U-Net 과 U-Net++ 모델이 수행한 세그멘테이션 결과 분석\n",
    "\n",
    "두 모델의 정량적, 정성적 성능을 비교해 봅시다. 시각화, IoU 계산 등을 체계적으로 시도해 보면 차이를 발견하실 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 준비한 모델을 불러오려면 아래 주석을 해제하세요\n",
    "model_path = dir_path + '/seg_model_unet.h5' \n",
    "\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, preproc, image_path, output_path):\n",
    "    # TODO: image_path로 입력된 이미지를 입력받아 preprocess를 해서 model로 infernece한 결과를 시각화하고 \n",
    "    # 이를 output_path에 저장하는 함수를 작성해주세요.\n",
    "    origin_img = imread(image_path)\n",
    "    data = {\"image\":origin_img}\n",
    "    processed = preproc(**data)\n",
    "    output = model(np.expand_dims(processed[\"image\"]/255,axis=0))\n",
    "    output = (output[0].numpy()>0.5).astype(np.uint8).squeeze(-1)*255  #0.5라는 threshold를 변경하면 도로인식 결과범위가 달라집니다.\n",
    "    output = Image.fromarray(output)\n",
    "    background = Image.fromarray(origin_img).convert('RGBA')\n",
    "    output = output.resize((origin_img.shape[1], origin_img.shape[0])).convert('RGBA')\n",
    "    output = Image.blend(background, output, alpha=0.5)\n",
    "    output.show()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성한 뒤에는 시각화한 결과를 눈으로 확인해봅시다!\n",
    "i = 1    # i값을 바꾸면 테스트용 파일이 달라집니다. \n",
    "get_output(\n",
    "     model, \n",
    "     test_preproc,\n",
    "     image_path=dir_path + f'/image_2/00{str(i).zfill(4)}_10.png',\n",
    "     output_path=dir_path + f'./result_{str(i).zfill(3)}.png'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou_score(target, prediction):\n",
    "    intersection = np.logical_and(target, prediction)\n",
    "    union = np.logical_or(target, prediction)\n",
    "    iou_score = float(np.sum(intersection)) / float(np.sum(union))\n",
    "    print('IoU : %f' % iou_score )\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, preproc, image_path, output_path, label_path):\n",
    "    origin_img = imread(image_path)\n",
    "    data = {\"image\":origin_img}\n",
    "    processed = preproc(**data)\n",
    "    output = model(np.expand_dims(processed[\"image\"]/255,axis=0))\n",
    "    output = (output[0].numpy()>=0.5).astype(np.uint8).squeeze(-1)*255  #0.5라는 threshold를 변경하면 도로인식 결과범위가 달라집니다.\n",
    "    prediction = output/255   # 도로로 판단한 영역\n",
    "    \n",
    "    output = Image.fromarray(output)\n",
    "    background = Image.fromarray(origin_img).convert('RGBA')\n",
    "    output = output.resize((origin_img.shape[1], origin_img.shape[0])).convert('RGBA')\n",
    "    output = Image.blend(background, output, alpha=0.5)\n",
    "    output.show()   # 도로로 판단한 영역을 시각화!\n",
    "     \n",
    "    if label_path:   \n",
    "        label_img = imread(label_path)\n",
    "        label_data = {\"image\":label_img}\n",
    "        label_processed = preproc(**label_data)\n",
    "        label_processed = label_processed[\"image\"]\n",
    "        target = (label_processed == 7).astype(np.uint8)*1   # 라벨에서 도로로 기재된 영역\n",
    "\n",
    "        return output, prediction, target\n",
    "    else:\n",
    "        return output, prediction, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성한 뒤에는 시각화한 결과를 눈으로 확인해봅시다!\n",
    "i = 1    # i값을 바꾸면 테스트용 파일이 달라집니다. \n",
    "output, prediction, target = get_output(\n",
    "     model, \n",
    "     test_preproc,\n",
    "     image_path=dir_path + f'/image_2/00{str(i).zfill(4)}_10.png',\n",
    "     output_path=dir_path + f'./result_{str(i).zfill(3)}.png',\n",
    "     label_path=dir_path + f'/semantic/00{str(i).zfill(4)}_10.png'\n",
    " )\n",
    "\n",
    "calculate_iou_score(target, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
