{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "inappropriate-baker",
   "metadata": {},
   "source": [
    "## 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-azerbaijan",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automated-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-mustang",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ultimate-alpha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-victim",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-switch",
   "metadata": {},
   "source": [
    "가사에 포함된 Verse, Chorus와 같은 요소들은 가사 내용이 아니므로 제외하고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "viral-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장\n",
    "     #sentence에 [Verse 1]과 같이 []로만 구성되어 있는 문장을 삭제한다. ([] 데이터 제거.1)\n",
    "    if re.match(r'\\[[^)]*\\]', sentence): continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eastern-adolescent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 1\n",
    "    # 문장에 가사와 함께 포함된 [Chorus]와 같은 요소들을 제거한다. ([] 데이터 제거.2)\n",
    "    sentence = re.sub(r'\\[[^)]*\\]', '', sentence)\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "theoretical-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150911\n"
     ]
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if re.match(r'\\[[^)]*\\]', sentence): continue\n",
    "    if len(sentence.split(\" \")) >= 12 : continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "novel-morocco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   43   64 ...    0    0    0]\n",
      " [   2    9 2958 ...    0    0    0]\n",
      " [   2   43   64 ...    0    0    0]\n",
      " ...\n",
      " [   2  551   20 ...    0    0    0]\n",
      " [   2  121   36 ...    3    0    0]\n",
      " [   2    5   23 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fb0b0a349d0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> they come from everywhere <end>',\n",
       " '<start> a longing to be free <end>',\n",
       " '<start> they come to join us here <end>',\n",
       " '<start> from sea to shining sea and they all have a dream <end>',\n",
       " '<start> as people always will <end>',\n",
       " '<start> to be safe and warm <end>',\n",
       " '<start> instead of opening the gate <end>',\n",
       " '<start> aw , let s turn this thing around <end>',\n",
       " '<start> before it gets too late <end>',\n",
       " '<start> it s up to me and you <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=14)\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "successful-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sophisticated-barrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  43  64  73 700   3   0   0   0   0   0   0   0]\n",
      "[ 43  64  73 700   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-intellectual",
   "metadata": {},
   "source": [
    "### Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "laden-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accomplished-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (120728, 13)\n",
      "Target Train: (120728, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-blade",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eight-anderson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 13), (256, 13)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12,000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12,001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "permanent-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "turned-cincinnati",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 13, 12001), dtype=float32, numpy=\n",
       "array([[[-8.94301193e-05, -6.29786655e-06, -5.16679611e-05, ...,\n",
       "         -6.24375461e-05,  9.96769595e-05, -9.46724685e-05],\n",
       "        [ 8.74621037e-07, -2.85960530e-04, -3.98364151e-04, ...,\n",
       "         -1.66154859e-04,  1.53619927e-04,  7.93450454e-05],\n",
       "        [-1.43335114e-04, -3.78931494e-04, -6.04001980e-04, ...,\n",
       "         -2.64297938e-04,  7.82661227e-05,  3.52073112e-05],\n",
       "        ...,\n",
       "        [ 8.77041603e-05, -1.34525006e-03,  2.04397402e-05, ...,\n",
       "         -1.23841874e-03,  5.31490019e-04, -1.35256210e-03],\n",
       "        [ 2.44222872e-04, -1.68232038e-03,  2.13212352e-05, ...,\n",
       "         -1.73617085e-03,  6.21623185e-04, -1.71871926e-03],\n",
       "        [ 4.28505824e-04, -1.99870998e-03, -1.97370483e-07, ...,\n",
       "         -2.19845911e-03,  7.34861300e-04, -2.02441285e-03]],\n",
       "\n",
       "       [[-8.94301193e-05, -6.29786655e-06, -5.16679611e-05, ...,\n",
       "         -6.24375461e-05,  9.96769595e-05, -9.46724685e-05],\n",
       "        [-5.15790191e-04, -1.60296840e-05, -2.03961230e-04, ...,\n",
       "          3.71385722e-05,  1.32119632e-04,  3.79577978e-06],\n",
       "        [-8.03549949e-04, -2.53068865e-04, -3.73287010e-04, ...,\n",
       "         -1.23896112e-04,  2.02983429e-04,  1.07895139e-04],\n",
       "        ...,\n",
       "        [-7.41275668e-04, -8.70000164e-04, -3.69093381e-04, ...,\n",
       "         -3.23535613e-04, -2.56149040e-04, -1.57132381e-04],\n",
       "        [-6.66877895e-04, -1.27444160e-03, -3.30448005e-04, ...,\n",
       "         -8.45782459e-04, -1.59036194e-04, -6.36589830e-04],\n",
       "        [-5.32440958e-04, -1.67714234e-03, -3.13483091e-04, ...,\n",
       "         -1.38217839e-03, -1.71400516e-05, -1.08923588e-03]],\n",
       "\n",
       "       [[-8.94301193e-05, -6.29786655e-06, -5.16679611e-05, ...,\n",
       "         -6.24375461e-05,  9.96769595e-05, -9.46724685e-05],\n",
       "        [-6.08983146e-06,  4.82223331e-05,  1.10912901e-04, ...,\n",
       "          7.71755367e-05,  5.60168228e-05,  4.94610285e-05],\n",
       "        [-1.92867024e-04,  2.84082234e-05,  3.77280026e-04, ...,\n",
       "          1.41107434e-04, -2.07752935e-04,  1.36432081e-05],\n",
       "        ...,\n",
       "        [ 2.39474655e-04,  2.41767988e-03,  3.12185381e-04, ...,\n",
       "         -1.31691433e-03, -7.00215634e-04, -3.82379716e-04],\n",
       "        [ 2.44496390e-04,  1.81977567e-03,  1.38963456e-04, ...,\n",
       "         -1.82851159e-03, -4.54541674e-04, -6.85264706e-04],\n",
       "        [ 2.71833007e-04,  1.13147683e-03, -3.56513519e-05, ...,\n",
       "         -2.35237507e-03, -1.89491198e-04, -1.01901242e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-8.94301193e-05, -6.29786655e-06, -5.16679611e-05, ...,\n",
       "         -6.24375461e-05,  9.96769595e-05, -9.46724685e-05],\n",
       "        [ 2.71191129e-05, -2.46455777e-04, -3.98775737e-05, ...,\n",
       "         -2.58034910e-04,  2.81813525e-04,  1.53328965e-05],\n",
       "        [-1.02005397e-05, -2.67081108e-04,  2.20810311e-04, ...,\n",
       "         -2.45269097e-04,  1.48426829e-04,  6.77235657e-05],\n",
       "        ...,\n",
       "        [ 5.09956793e-04, -2.74043763e-03, -1.72899847e-04, ...,\n",
       "         -3.05318693e-03,  5.67356066e-04, -2.15055840e-03],\n",
       "        [ 7.08202191e-04, -2.99780560e-03, -2.46593379e-04, ...,\n",
       "         -3.35003273e-03,  7.66621553e-04, -2.35684332e-03],\n",
       "        [ 9.13960394e-04, -3.22033744e-03, -3.17104685e-04, ...,\n",
       "         -3.59822484e-03,  9.67003230e-04, -2.52465717e-03]],\n",
       "\n",
       "       [[-8.94301193e-05, -6.29786655e-06, -5.16679611e-05, ...,\n",
       "         -6.24375461e-05,  9.96769595e-05, -9.46724685e-05],\n",
       "        [-1.16649055e-04,  3.89989291e-04,  1.76328118e-04, ...,\n",
       "         -1.42223740e-04,  4.20831475e-06,  2.45463925e-05],\n",
       "        [ 6.97626092e-05,  5.32868202e-04,  3.15777404e-04, ...,\n",
       "          5.31027326e-05, -1.97271132e-04,  2.59101071e-04],\n",
       "        ...,\n",
       "        [ 1.10848749e-03, -1.30413519e-03,  2.52505270e-05, ...,\n",
       "         -4.63399221e-04,  6.96999137e-04,  6.93675422e-04],\n",
       "        [ 1.15723663e-03, -1.36528839e-03,  2.95742328e-04, ...,\n",
       "         -5.28605888e-04,  5.43239468e-04,  5.16080006e-04],\n",
       "        [ 1.02585414e-03, -1.61606364e-03,  3.84490268e-04, ...,\n",
       "         -8.32016871e-04,  4.38664778e-04,  7.78091126e-05]],\n",
       "\n",
       "       [[-8.94301193e-05, -6.29786655e-06, -5.16679611e-05, ...,\n",
       "         -6.24375461e-05,  9.96769595e-05, -9.46724685e-05],\n",
       "        [-1.78592127e-05,  1.87727914e-04, -1.53763802e-04, ...,\n",
       "          1.29221764e-04,  1.18727308e-04,  3.60117047e-05],\n",
       "        [-3.70410475e-04,  2.31232436e-04,  4.36434711e-05, ...,\n",
       "          2.89543124e-04,  4.92698746e-05,  1.29566470e-04],\n",
       "        ...,\n",
       "        [ 6.64680556e-04, -1.13822683e-03,  7.92732928e-04, ...,\n",
       "         -5.25610987e-04, -6.03631139e-04, -1.11144118e-03],\n",
       "        [ 7.91583036e-04, -1.58001028e-03,  6.73816947e-04, ...,\n",
       "         -1.19569828e-03, -4.46355582e-04, -1.53209816e-03],\n",
       "        [ 9.25369968e-04, -1.98262348e-03,  5.43515955e-04, ...,\n",
       "         -1.81434012e-03, -2.51627906e-04, -1.88940892e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dependent-arbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dirty-charles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "589/589 [==============================] - 174s 291ms/step - loss: 4.1288\n",
      "Epoch 2/15\n",
      "589/589 [==============================] - 173s 293ms/step - loss: 3.1841\n",
      "Epoch 3/15\n",
      "589/589 [==============================] - 173s 293ms/step - loss: 2.9882\n",
      "Epoch 4/15\n",
      "589/589 [==============================] - 173s 293ms/step - loss: 2.8284\n",
      "Epoch 5/15\n",
      "589/589 [==============================] - 172s 292ms/step - loss: 2.7038\n",
      "Epoch 6/15\n",
      "589/589 [==============================] - 172s 292ms/step - loss: 2.5934\n",
      "Epoch 7/15\n",
      "589/589 [==============================] - 172s 292ms/step - loss: 2.4901\n",
      "Epoch 8/15\n",
      "589/589 [==============================] - 171s 291ms/step - loss: 2.3909\n",
      "Epoch 9/15\n",
      "589/589 [==============================] - 172s 291ms/step - loss: 2.2958\n",
      "Epoch 10/15\n",
      "589/589 [==============================] - 171s 290ms/step - loss: 2.2158\n",
      "Epoch 11/15\n",
      "589/589 [==============================] - 172s 291ms/step - loss: 2.1329\n",
      "Epoch 12/15\n",
      "589/589 [==============================] - 171s 290ms/step - loss: 2.0589\n",
      "Epoch 13/15\n",
      "589/589 [==============================] - 171s 290ms/step - loss: 1.9890\n",
      "Epoch 14/15\n",
      "589/589 [==============================] - 171s 290ms/step - loss: 1.9165\n",
      "Epoch 15/15\n",
      "589/589 [==============================] - 171s 291ms/step - loss: 1.8588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb0aedde1d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-sierra",
   "metadata": {},
   "source": [
    "텍스트 생성모델의 validation loss를 2.2 이하로 줄이기 위해 Epoch를 조정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "armed-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "composed-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "according-tongue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-vacuum",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-ballet",
   "metadata": {},
   "source": [
    "문제에서 권장하는 데이터 개수인 124,960개의 데이터 이하로 줄이기 위해, token의 개수를 15개 이하로 줄이는 것 외에 시도해 본 것\n",
    "- sentence의 수집 과정에서 단어의 수를 제한했다.\n",
    "- [Verse]와 같은 Sentence 자체가 [ ]로 이루어진 요소를 제거했다. ([ ] 데이터 제거.1)\n",
    "- [Chorus]와 같이 Sentence에 포함된 [ ]로 이루어진 요소를 제거했다. ([ ] 데이터 제거.2)\n",
    "\n",
    "시도해 보려고 했지만 실패한 것\n",
    "- 가사 내부에서 같은 Sentence가 여러 번 반복된 경우를 ndarray의 행에서 삭제하고자 시도해 보았지만, 아직 numpy 배열에 익숙하지 않아서인지 성공하지 못했다.\n",
    "\n",
    ": sentence를 이루는 단어의 수를 제한하는 것보다는 불필요한 요소를 줄이는 방향으로 데이터의 양을  줄이고자 하였지만, 생각보다 데이터의 수를 많이 줄이지 못해 아쉬웠고, 내가 어떤 부분에서 부족한지를 좀 더 알게 되어 흥미롭게 프로젝트를 진행할 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-cooling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
